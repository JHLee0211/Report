### Linear Regression의 cost 최소화 알고리즘의 원리

---

```
	Hypothesis : H(x) = Wx + b
```



- Cost Function : cost(W,b) = 1/m * (i=1~m) ∑ (H(xi) - y(i))^2
  - 이 때 cost를 최소화하는 W와 b의 값을 가진 데이터를 구하는 것이 Linear Regression의 목적

		Simplified Hypothesis : H(x) = Wx
		cost(W) = 1/m * (i=1~m) ∑ (Wx(i) - y(i))^2
		Wx = H(x) 라고 생각!!



#### algorithm (경사를 따른 감소 알고리즘)

  - Minimize cost function, 많은 minimization problem에 이용

  - cost(W,b)에서 가장 minimize된 값을 찾으며, cost(w1,w2,...) 의 general function에도 적용가능

    - x축이 W, y축이 cost인 그래프

    - y값이 작아지는 경사를 따라 x값을 움직이며 기울기가 0인 지점을 찾기 >> 이 지점이 cost 최소값

    - 어느 위치에서나 시작 가능

    - 경사도를 구하는 법 : 미분

    ##### Cost(W)

    ```
      Cost(W) = 1/(2*m) * (i=1~m) ∑ (Wx(i) - y(i))^2
      	>> 미분을 편하게 하기 위해 맨 앞에 2를 나눠줌
    ```

    ##### W 

    ```
      W := W - α*(W에 대한 cost(W)의 미분값)
        >> cost(W)값을 미분하여 기울기값을 얻고, 기울기의 음/양에 따라 움직이는 방향을 알기 위함
        
         = W - α* 1/m * (i=1~m) ∑ (Wx(i) - y(i))*x(i)
        		>> α:learning rate(상수)
        		
    - 위 계산을 여러 번 실행시키며 W값이 바뀌면 결국 W값은 최소의 cost를 갖는 W로 이동
    ```

    ##### Cost(W)

    ```
      Gradient descent algorithm : W := W - α* 1/m * (i=1~m) ∑ (Wx(i) - y(i))*x(i)
    ```



#### Convex Function 

  - ##### 3차원 그래프에서, 밥그릇 모양으로 가운데가 볼록하게 들어간 함수

  - W,b에 대한 함수로 cost function을 만들 때 3차 함수가 만들어질 경우, Gradient descent algorithm이 통하지 않을 것

- 따라서, 항상 cost(W,b)함수가 Convex function 형태인지 확인해야 함!

  

### Linear Regression 의 cost 최소화의 TensorFlow 구현 

---


* 코드 (C:\Users\cjstn\AppData\Local\Programs\Python\Python36 안의 hello/tensorPlot/tensorPrac1,2,3)

* tensorPlot.py

  ```python
  import tensorflow as tf
  import matplotlib.pyplot as plt
  
  X = [1,2,3]
  Y = [1,2,3]
  
  W = tf.placeholder(tf.float32)	 # W값을 임의대로 바꾸어 가면서 보겠음
  #Our hypothesis for linear model X*W
  hypothesis = X*W
  
  # cost/Loss function
  
  cost = tf.reduce_mean(tf.square(hypothesis - Y)) # hypothesis와 Y값의 차를 제곱하며 평균냄(cost 구하는 식)
  
  # Launch the graph in a session
  
  sess = tf.Session()
  
  # Initializes global variables in the graph.
  
  sess.run(tf.global_variables_initializer())	 # session을 열고 variable을 초기화
  
  # variables for plotting cost function
  
  W_val = []
  cost_val = []			# W, cost 값을 저장할 _val list 생성
  
  for i in range(-30, 50):						# range -30 ~ 50
      feed_W = i * 0.1							# W를 -3~5 정도로 움직이겠다
      curr_cost, curr_W = sess.run([cost,W], feed_dict={W: feed_W})	# W값을 위의 값으로 feed_W 하고, cost와 W의 값을 curr_ 에 넣어 저장
      W_val.append(curr_W)						
      cost_val.append(curr_cost)		# curr에 넣은 W와 cost를 _val list에 넣음
  
  # Show the cost function
  
  plt.plot(W_val, cost_val)		# 가로가 W, 세로가 cost(W) 인 그래프 생성
  plt.show()		# W값이 1일 때 cost(W)가 최소인 것을 알 수 있는 2차원 그래프 생성
  
  
  ```

  

#### Gradient descent

- Cost(W) = 1/m * (i=1→m) ∑ (W*x(i) - y(i))^2
  - 미분을 편하게 하기 위해 맨앞에 2를 나눠줌

	Gradient descent algorithm : W := W - α* 1/m * (i=1~m) ∑ (Wx(i) - y(i))*x(i)

- tensorPrac1.py (수동으로 cost를 minimize 하는 법)

```python
# Minimize: Gradient Descent using derivative: W-= Learning_rate * derivative

  learning_rate = 0.1
  gradient = tf.reduce_mean((W * X - Y) * X)		# gradient = 1/m * (i=1~m) ∑ (Wx(i) - y(i))*x(i)
  descent = W - learning_rate * gradient		# descent = W - α* gradient
  update = W.assian(descent)				# update : W = descenct


  sess = tf.Session()
  sess.run(tf.global_variables_initializer())		# 이것 또한 그래프이기 때문에 session을 염

  for step in range(21):
	sess.run(update, feed_dict={X: x_data, Y: y_data})				# 위의 update를 켜고, X와 Y에 값을 feed함
	print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))  	# cost와 sess.run(W)의 W값을 확인함
```



## 알고리즘 (프로그래머스)

### 완전탐색

 - ##### 42841. 숫자야구

```java
import java.util.*;

public class Solution_prog_42841_숫자야구 {	
    public static int[] arr;
    public static int[][] bb;
    public static int answer;
	public static void main(String[] args) {
		int [][] baseball=new int[][] {{123,1,1},{356,1,0},{327,2,0},{489,0,1}};
		System.out.println(solution(baseball));
	}
    public static int solution(int[][] baseball) {
        answer = 0;
        arr=new int[3];
        bb=baseball;
        for(int i=1; i<8; i++){
            arr[0]=i;
            for(int j=i+1; j<9; j++){
                arr[1]=j;
                for(int k=j+1; k<10; k++){
                    arr[2]=k;
                    perm(arr.length,0);
                }
            }
        }
        return answer;
    }
    public static void perm(int n, int k){
        if(n==k){
        	int chk=0;
            for(int i=0; i<bb.length; i++){
                chk+=check(i);
            }
            if(chk==bb.length) answer++;
        }else{
            for(int i=k; i<n; i++){
                int temp=arr[i];
                arr[i]=arr[k];
                arr[k]=temp;
                perm(n,k+1);
                temp=arr[i];
                arr[i]=arr[k];
                arr[k]=temp;
            }
        }
    }
    public static int check(int n){
        String s =""+bb[n][0];
        int str = bb[n][1];
        int ball=bb[n][2];
        //System.out.println(s+ " " +str + " " +ball + " "+Arrays.toString(arr));
        for(int i=0; i<s.length(); i++){
	        for(int j=0; j<s.length(); j++){
	            if(s.charAt(i)-'0' == arr[j]) {
	                if(i==j) str--;
	                else ball--;
	                break;
	            }
	        }
        }
        if(ball==0 && str==0){
        	return 1;
        }
        return 0;
    }
}
```



- ##### 42842. 카펫

```java
import java.util.*;

public class Solution_prog_42842_카펫 {
	public static void main(String[] args) {
		System.out.println(Arrays.toString(solution(10,2)));
	}
    public static int[] solution(int brown, int red) {
        int[] answer = new int[2];
        int i;
        for(i=3; i*i<=brown+red; i++){
            if((2*i-(Math.pow(i,2)-brown/2*i))==(brown+red)){
                answer[1]=i;
                answer[0]=(brown+red)/i;
                break;
            }
        }
        return answer;
    }
}
```